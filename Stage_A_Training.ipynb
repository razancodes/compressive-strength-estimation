{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage A: Early-Age Concrete Compressive Strength Prediction\n## ML Baseline Training Notebook (Colab GPU)\n\n**Project:** Data-Driven Early-Age Concrete Strength Prediction\n**Phase:** Stage A â€” Machine Learning Baseline on UCI Concrete Dataset\n\n**Models:** XGBoost Â· CatBoost Â· LightGBM (Optuna-optimized)\n**Validation:** 5-fold Nested Cross-Validation (unbiased estimates)\n**Explainability:** SHAP TreeExplainer analysis\n\n---\n### How to use this notebook:\n1. Set **Runtime â†’ Change runtime type â†’ T4 GPU** (recommended but optional)\n2. Run cells sequentially from top to bottom\n3. Upload your `Concrete_Data` CSV when prompted (first session only)\n4. After training completes, download the `models/` ZIP from the final cell\n\n### âš¡ Checkpoint / Resume (for free-tier 2-hour limit)\n- All results are **saved to Google Drive after every modelÃ—subset combo**\n- If your runtime disconnects, just **reconnect and re-run all cells** â€” completed combos are automatically skipped\n- You can split training across multiple sessions with zero repeated work\n\n> **Quick test:** Set `N_TRIALS = 10` in the Configuration cell for a ~15 min run\n> **Full run:** Set `N_TRIALS = 100` for publication-grade results (~2-4 hours on GPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies & Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q xgboost catboost lightgbm optuna shap --upgrade\nprint(\"âœ… All packages installed successfully!\")\n\n# Mount Google Drive for persistent storage\nfrom google.colab import drive\ndrive.mount('/content/drive')\nprint(\"âœ… Google Drive mounted.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport os\nimport time\nimport warnings\nimport shutil\nimport subprocess\n\nfrom sklearn.model_selection import KFold, GroupKFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\nimport optuna\nfrom optuna.samplers import TPESampler\nimport shap\nimport joblib\n\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n# CONFIGURATION â€” Adjust these values before running\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nN_TRIALS = 100          # Optuna trials per model-subset (10 for quick test)\nN_OUTER_FOLDS = 5       # Outer CV folds for nested cross-validation\nRANDOM_SEED = 42        # Global seed for reproducibility\nSUBSET_NAMES = ['EA1', 'EA7', 'EA14', 'Full']  # Remove any to skip\n\n# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nnp.random.seed(RANDOM_SEED)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# â”€â”€ Persistent paths (Google Drive survives disconnects) â”€â”€\nDRIVE_DIR = '/content/drive/MyDrive/Stage_A_Concrete'\nos.makedirs(DRIVE_DIR, exist_ok=True)\nos.makedirs(f'{DRIVE_DIR}/outputs', exist_ok=True)\nos.makedirs(f'{DRIVE_DIR}/models', exist_ok=True)\nos.makedirs(f'{DRIVE_DIR}/checkpoints', exist_ok=True)\n\n# Also keep local copies for speed\nos.makedirs('outputs', exist_ok=True)\nos.makedirs('models', exist_ok=True)\n\n# GPU detection\ntry:\n    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n    GPU_AVAILABLE = result.returncode == 0\nexcept FileNotFoundError:\n    GPU_AVAILABLE = False\n\nprint(f\"GPU Available: {GPU_AVAILABLE}\")\nprint(f\"Optuna trials per model-subset: {N_TRIALS}\")\nprint(f\"Subsets to evaluate: {SUBSET_NAMES}\")\n\nif GPU_AVAILABLE:\n    !nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n    print(\"\\nâœ… GPU acceleration will be used for XGBoost and CatBoost\")\nelse:\n    print(\"\\nâš ï¸  No GPU detected. Training will use CPU (slower).\")\n    print(\"    Set Runtime â†’ Change runtime type â†’ T4 GPU for speedup.\")\n\nprint(f\"\\nxgboost={xgb.__version__}, catboost={CatBoostRegressor.__module__}, lightgbm={lgb.__version__}\")\nprint(f\"optuna={optuna.__version__}, shap={shap.__version__}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preprocessing\n\nUpload the **Concrete_Data CSV** file when prompted.\nExpected: 1030 rows Ã— 9 columns (8 inputs + 1 target).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import files\n\n# Check if data already exists on Drive (from a previous session)\ndrive_csv = f'{DRIVE_DIR}/data_clean.csv'\nif os.path.exists(drive_csv):\n    print(f\"âœ… Found saved dataset on Drive: {drive_csv}\")\n    df_raw = pd.read_csv(drive_csv)\n    filename = drive_csv\n    print(f\"   Loaded: {df_raw.shape[0]} rows Ã— {df_raw.shape[1]} columns\")\nelse:\n    print(\"ğŸ“ Upload your Concrete_Data CSV file:\")\n    uploaded = files.upload()\n    filename = list(uploaded.keys())[0]\n    print(f\"\\nUploaded: {filename}\")\n    df_raw = pd.read_csv(filename)\n\n# Standardize column names\nEXPECTED_COLS = [\n    'Cement', 'Blast_Furnace_Slag', 'Fly_Ash', 'Water',\n    'Superplasticizer', 'Coarse_Aggregate', 'Fine_Aggregate',\n    'Age', 'Compressive_Strength'\n]\n\n# Try to map known long column names to short names\nCOLUMN_MAP = {\n    'Cement (component 1)(kg in a m^3 mixture)': 'Cement',\n    'Blast Furnace Slag (component 2)(kg in a m^3 mixture)': 'Blast_Furnace_Slag',\n    'Fly Ash (component 3)(kg in a m^3 mixture)': 'Fly_Ash',\n    'Water  (component 4)(kg in a m^3 mixture)': 'Water',\n    'Superplasticizer (component 5)(kg in a m^3 mixture)': 'Superplasticizer',\n    'Coarse Aggregate  (component 6)(kg in a m^3 mixture)': 'Coarse_Aggregate',\n    'Fine Aggregate (component 7)(kg in a m^3 mixture)': 'Fine_Aggregate',\n    'Age (day)': 'Age',\n}\n# Handle target column (may have trailing space)\nfor col in df_raw.columns:\n    stripped = col.strip()\n    if 'compressive' in stripped.lower() and 'strength' in stripped.lower():\n        COLUMN_MAP[col] = 'Compressive_Strength'\n\nrename_dict = {}\nfor col in df_raw.columns:\n    stripped = col.strip()\n    if col in COLUMN_MAP:\n        rename_dict[col] = COLUMN_MAP[col]\n    elif stripped in COLUMN_MAP:\n        rename_dict[col] = COLUMN_MAP[stripped]\n\nif rename_dict:\n    df_raw = df_raw.rename(columns=rename_dict)\n\n# Fallback: if columns still don't match, assign by position\nif not all(c in df_raw.columns for c in EXPECTED_COLS):\n    if len(df_raw.columns) == 9:\n        print(\"âš ï¸  Column names not recognized. Assigning by position.\")\n        df_raw.columns = EXPECTED_COLS\n    else:\n        raise ValueError(f\"Expected 9 columns, got {len(df_raw.columns)}: {list(df_raw.columns)}\")\n\n# Clean\ndf_raw = df_raw.drop_duplicates()\nassert df_raw.isnull().sum().sum() == 0, f\"Found missing values:\\n{df_raw.isnull().sum()}\"\n\nprint(f\"\\nâœ… Dataset loaded: {df_raw.shape[0]} rows Ã— {df_raw.shape[1]} columns\")\nprint(f\"\\nTarget statistics (Compressive Strength, MPa):\")\nprint(df_raw['Compressive_Strength'].describe().round(2))\nprint(f\"\\nAge values: {sorted(df_raw['Age'].unique())}\")\n\n# Save a clean copy for reference (local + Drive)\ndf_raw.to_csv('data_clean.csv', index=False)\ndf_raw.to_csv(f'{DRIVE_DIR}/data_clean.csv', index=False)\nprint(\"   Data backed up to Google Drive.\")\ndf_raw.head(8)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n\n14 domain-derived features based on concrete engineering theory:\n- **Binder system (5):** Binder, W/B ratio, GGBS ratio, Fly Ash ratio, SCM ratio\n- **Aggregate (3):** Total Aggregate, Fine Agg ratio, Agg/Binder ratio\n- **Admixture (1):** SP per binder\n- **Temporal (5):** log(Age), âˆšAge, age phase indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def engineer_features(df):\n    \"\"\"Create 14 derived features from raw mix design parameters.\"\"\"\n    d = df.copy()\n\n    # Binder system\n    d['Binder'] = d['Cement'] + d['Blast_Furnace_Slag'] + d['Fly_Ash']\n    d['W_B_ratio'] = d['Water'] / d['Binder']\n    d['GGBS_ratio'] = d['Blast_Furnace_Slag'] / d['Binder']\n    d['FlyAsh_ratio'] = d['Fly_Ash'] / d['Binder']\n    d['SCM_ratio'] = (d['Blast_Furnace_Slag'] + d['Fly_Ash']) / d['Binder']\n\n    # Aggregate\n    d['Total_Aggregate'] = d['Coarse_Aggregate'] + d['Fine_Aggregate']\n    d['Fine_Agg_ratio'] = d['Fine_Aggregate'] / d['Total_Aggregate']\n    d['Agg_Binder_ratio'] = d['Total_Aggregate'] / d['Binder']\n\n    # Admixture\n    d['SP_per_binder'] = d['Superplasticizer'] / d['Binder']\n\n    # Temporal\n    d['log_Age'] = np.log1p(d['Age'])\n    d['sqrt_Age'] = np.sqrt(d['Age'])\n    d['Age_very_early'] = (d['Age'] <= 3).astype(int)\n    d['Age_early'] = ((d['Age'] > 3) & (d['Age'] <= 7)).astype(int)\n    d['Age_standard'] = ((d['Age'] > 7) & (d['Age'] <= 28)).astype(int)\n\n    # Handle inf/nan from division by zero\n    d.replace([np.inf, -np.inf], np.nan, inplace=True)\n    d.fillna(0, inplace=True)\n\n    return d\n\ndf_feat = engineer_features(df_raw)\nfeature_cols = [c for c in df_feat.columns if c != 'Compressive_Strength']\nprint(f\"âœ… {len(feature_cols)} features: {feature_cols}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Target distribution\naxes[0].hist(df_raw['Compressive_Strength'], bins=40, edgecolor='white', color='#2196F3', alpha=0.85)\naxes[0].set_xlabel('Compressive Strength (MPa)')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Target Distribution', fontweight='bold')\naxes[0].grid(alpha=0.2)\n\n# Strength vs Age\naxes[1].scatter(df_raw['Age'], df_raw['Compressive_Strength'], alpha=0.4, s=25, c='#FF5722', edgecolors='white', linewidths=0.3)\naxes[1].set_xlabel('Age (days)')\naxes[1].set_ylabel('Compressive Strength (MPa)')\naxes[1].set_title('Strength vs Curing Age', fontweight='bold')\naxes[1].grid(alpha=0.2)\n\n# W/B ratio vs Strength\naxes[2].scatter(df_feat['W_B_ratio'], df_raw['Compressive_Strength'], alpha=0.4, s=25, c='#4CAF50', edgecolors='white', linewidths=0.3)\naxes[2].set_xlabel('Water-to-Binder Ratio')\naxes[2].set_ylabel('Compressive Strength (MPa)')\naxes[2].set_title('Strength vs W/B Ratio', fontweight='bold')\naxes[2].grid(alpha=0.2)\n\nplt.tight_layout()\nplt.savefig('outputs/eda_overview.png', dpi=200, bbox_inches='tight')\nplt.show()\n\n# Correlation heatmap\nraw_cols = ['Cement', 'Blast_Furnace_Slag', 'Fly_Ash', 'Water',\n            'Superplasticizer', 'Coarse_Aggregate', 'Fine_Aggregate',\n            'Age', 'Compressive_Strength']\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(df_raw[raw_cols].corr(), annot=True, fmt='.2f', cmap='coolwarm',\n            center=0, ax=ax, linewidths=0.5, annot_kws={'fontsize': 9})\nax.set_title('Feature Correlation Matrix', fontweight='bold', fontsize=14)\nplt.tight_layout()\nplt.savefig('outputs/eda_correlation.png', dpi=200, bbox_inches='tight')\nplt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Early-Age Subset Creation\n\n| Subset | Age Range | Purpose |\n|--------|-----------|---------|\n| EA1    | â‰¤ 3 days  | Formwork removal decisions |\n| EA7    | â‰¤ 7 days  | Construction sequencing |\n| EA14   | â‰¤ 14 days | Quality assurance |\n| Full   | All ages  | Baseline comparison |\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ALL_SUBSETS = {\n    'EA1':  df_feat[df_feat['Age'] <= 3].copy(),\n    'EA7':  df_feat[df_feat['Age'] <= 7].copy(),\n    'EA14': df_feat[df_feat['Age'] <= 14].copy(),\n    'Full': df_feat.copy(),\n}\n\n# Filter to only requested subsets\nsubsets = {k: v for k, v in ALL_SUBSETS.items() if k in SUBSET_NAMES}\n\nprint(f\"{'Subset':<8} {'Age Range':<12} {'Samples':>8}\")\nprint(\"-\" * 32)\nfor name, sdf in subsets.items():\n    age_range = f\"1-{int(sdf['Age'].max())}d\"\n    print(f\"{name:<8} {age_range:<12} {len(sdf):>8}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training Functions\n\nDefines:\n- Manual K-fold CV (CatBoost/sklearn compatibility)\n- Optuna objectives for XGBoost, CatBoost, LightGBM\n- Nested cross-validation pipeline\n- Model factory\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Manual CV (for CatBoost sklearn compatibility) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef manual_cv_rmse(model_class, params, X, y, n_splits=5):\n    \"\"\"Manual KFold CV â€” avoids CatBoost/sklearn __sklearn_tags__ issue.\"\"\"\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n    scores = []\n    X_np = X.values if hasattr(X, 'values') else np.array(X)\n    y_np = y.values if hasattr(y, 'values') else np.array(y)\n\n    for train_idx, val_idx in kf.split(X_np):\n        model = model_class(**params)\n        model.fit(X_np[train_idx], y_np[train_idx])\n        y_pred = model.predict(X_np[val_idx])\n        scores.append(np.sqrt(mean_squared_error(y_np[val_idx], y_pred)))\n\n    return np.mean(scores)\n\n\n# â”€â”€ Optuna Objectives â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef objective_xgboost(trial, X, y):\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n        'random_state': RANDOM_SEED, 'n_jobs': -1,\n    }\n    if GPU_AVAILABLE:\n        params['tree_method'] = 'hist'\n        params['device'] = 'cuda'\n    model = xgb.XGBRegressor(**params)\n    scores = cross_val_score(model, X, y, cv=5,\n                             scoring='neg_root_mean_squared_error', n_jobs=-1)\n    return -scores.mean()\n\n\ndef objective_catboost(trial, X, y):\n    params = {\n        'depth': trial.suggest_int('depth', 4, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'iterations': trial.suggest_int('iterations', 500, 2000),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n        'random_strength': trial.suggest_float('random_strength', 0.0, 2.0),\n        'random_state': RANDOM_SEED, 'verbose': 0,\n    }\n    if GPU_AVAILABLE:\n        params['task_type'] = 'GPU'\n        params['devices'] = '0'\n    return manual_cv_rmse(CatBoostRegressor, params, X, y)\n\n\ndef objective_lightgbm(trial, X, y):\n    params = {\n        'num_leaves': trial.suggest_int('num_leaves', 15, 127),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n        'random_state': RANDOM_SEED, 'n_jobs': -1, 'verbose': -1,\n    }\n    model = lgb.LGBMRegressor(**params)\n    scores = cross_val_score(model, X, y, cv=5,\n                             scoring='neg_root_mean_squared_error', n_jobs=-1)\n    return -scores.mean()\n\n\nOBJECTIVE_MAP = {\n    'XGBoost':  objective_xgboost,\n    'CatBoost': objective_catboost,\n    'LightGBM': objective_lightgbm,\n}\n\n\n# â”€â”€ Model Factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef create_model(model_type, params):\n    \"\"\"Instantiate a model with best params + GPU config.\"\"\"\n    p = dict(params)\n    if model_type == 'XGBoost':\n        if GPU_AVAILABLE:\n            p['tree_method'] = 'hist'\n            p['device'] = 'cuda'\n        return xgb.XGBRegressor(**p, random_state=RANDOM_SEED, n_jobs=-1)\n    elif model_type == 'CatBoost':\n        if GPU_AVAILABLE:\n            p['task_type'] = 'GPU'\n            p['devices'] = '0'\n        return CatBoostRegressor(**p, random_state=RANDOM_SEED, verbose=0)\n    elif model_type == 'LightGBM':\n        return lgb.LGBMRegressor(**p, random_state=RANDOM_SEED, n_jobs=-1, verbose=-1)\n    else:\n        raise ValueError(f\"Unknown model: {model_type}\")\n\n\n# â”€â”€ Nested Cross-Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef nested_cv(X, y, model_type, n_outer=N_OUTER_FOLDS, n_trials=N_TRIALS):\n    \"\"\"Full nested CV: outer folds for evaluation, inner Optuna for HPO.\"\"\"\n    outer_cv = KFold(n_splits=n_outer, shuffle=True, random_state=RANDOM_SEED)\n    objective_fn = OBJECTIVE_MAP[model_type]\n\n    metrics = {'rmse': [], 'mae': [], 'r2': [], 'mape': [], 'max_err': []}\n    y_true_all, y_pred_all = [], []\n    best_params_per_fold = []\n\n    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n        print(f\"    Fold {fold+1}/{n_outer}\", end=\" ... \", flush=True)\n        t0 = time.time()\n\n        X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n        X_te, y_te = X.iloc[test_idx],  y.iloc[test_idx]\n\n        # Inner HPO\n        study = optuna.create_study(direction='minimize',\n                                    sampler=TPESampler(seed=RANDOM_SEED + fold))\n        study.optimize(lambda trial: objective_fn(trial, X_tr, y_tr),\n                       n_trials=n_trials, show_progress_bar=False)\n\n        best_p = study.best_params\n        best_params_per_fold.append(best_p)\n\n        # Train on outer train, predict on outer test\n        model = create_model(model_type, best_p)\n        model.fit(X_tr, y_tr)\n        y_pred = model.predict(X_te)\n\n        rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n        mae = mean_absolute_error(y_te, y_pred)\n        r2 = r2_score(y_te, y_pred)\n        mask = y_te != 0\n        mape = np.mean(np.abs((y_te[mask] - y_pred[mask.values]) / y_te[mask])) * 100 if mask.sum() > 0 else np.nan\n        max_e = np.max(np.abs(y_te.values - y_pred))\n\n        metrics['rmse'].append(rmse)\n        metrics['mae'].append(mae)\n        metrics['r2'].append(r2)\n        metrics['mape'].append(mape)\n        metrics['max_err'].append(max_e)\n        y_true_all.extend(y_te.values.tolist())\n        y_pred_all.extend(y_pred.tolist())\n\n        print(f\"RMSE={rmse:.2f}, RÂ²={r2:.4f} ({time.time()-t0:.0f}s)\")\n\n    return {\n        'RMSE_mean': np.mean(metrics['rmse']),  'RMSE_std': np.std(metrics['rmse']),\n        'MAE_mean':  np.mean(metrics['mae']),   'MAE_std':  np.std(metrics['mae']),\n        'R2_mean':   np.mean(metrics['r2']),    'R2_std':   np.std(metrics['r2']),\n        'MAPE_mean': np.nanmean(metrics['mape']), 'MAPE_std': np.nanstd(metrics['mape']),\n        'MaxErr_mean': np.mean(metrics['max_err']),\n        'y_true': y_true_all, 'y_pred': y_pred_all,\n        'best_params_per_fold': best_params_per_fold,\n    }\n\nprint(\"âœ… All training functions defined.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Baseline Models (Linear Regression + Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_results = []\n\nfor subset_name, subset_df in subsets.items():\n    X = subset_df[feature_cols]\n    y = subset_df['Compressive_Strength']\n\n    print(f\"\\nâ”€â”€ Baselines: {subset_name} ({len(X)} samples) â”€â”€\")\n\n    for name, model in [('LinearRegression', LinearRegression()),\n                        ('RandomForest', RandomForestRegressor(n_estimators=200, max_depth=10,\n                                                                random_state=RANDOM_SEED, n_jobs=-1))]:\n        cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n        rmses, maes, r2s = [], [], []\n        for tr_idx, te_idx in cv.split(X):\n            m = type(model)(**model.get_params())\n            m.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n            yp = m.predict(X.iloc[te_idx])\n            rmses.append(np.sqrt(mean_squared_error(y.iloc[te_idx], yp)))\n            maes.append(mean_absolute_error(y.iloc[te_idx], yp))\n            r2s.append(r2_score(y.iloc[te_idx], yp))\n\n        res = {'Subset': subset_name, 'Model': name,\n               'RMSE_mean': np.mean(rmses), 'RMSE_std': np.std(rmses),\n               'MAE_mean': np.mean(maes),   'MAE_std': np.std(maes),\n               'R2_mean': np.mean(r2s),     'R2_std': np.std(r2s)}\n        all_results.append(res)\n        print(f\"  {name:<20s} RMSE={res['RMSE_mean']:.3f}Â±{res['RMSE_std']:.3f}  RÂ²={res['R2_mean']:.4f}Â±{res['R2_std']:.4f}\")\n\nprint(\"\\nâœ… Baselines complete.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gradient Boosting Model Training\n\nRuns **nested cross-validation** for XGBoost, CatBoost, and LightGBM on each age subset.\nEach outer fold runs a fresh Optuna optimization (inner loop) â€” this gives **unbiased** performance estimates.\n\n> â± Expected time: ~15 min (quick/10 trials) Â· ~2-4 hours (full/100 trials) on T4 GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MODEL_TYPES = ['XGBoost', 'CatBoost', 'LightGBM']\nall_best_params = {}\nall_predictions = {}  # Store for plotting\n\n# â”€â”€ Load any existing checkpoints from Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ncheckpoint_dir = f'{DRIVE_DIR}/checkpoints'\ncompleted_combos = set()\n\nfor ckpt_file in os.listdir(checkpoint_dir):\n    if ckpt_file.endswith('.json'):\n        key = ckpt_file.replace('.json', '')\n        completed_combos.add(key)\n        with open(f'{checkpoint_dir}/{ckpt_file}', 'r') as f:\n            ckpt = json.load(f)\n        all_results.append(ckpt['result_row'])\n        all_best_params[key] = ckpt['best_params']\n        all_predictions[key] = ckpt['predictions']\n\nif completed_combos:\n    print(f\"âœ… Resumed {len(completed_combos)} completed checkpoints from Drive:\")\n    for c in sorted(completed_combos):\n        print(f\"   âœ“ {c}\")\n    print()\n\n# â”€â”€ Training loop with per-combo checkpointing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntotal_combos = len(subsets) * len(MODEL_TYPES)\ncombo_idx = 0\npipeline_start = time.time()\n\nfor subset_name, subset_df in subsets.items():\n    X = subset_df[feature_cols]\n    y = subset_df['Compressive_Strength']\n\n    for model_type in MODEL_TYPES:\n        combo_idx += 1\n        key = f\"{model_type}_{subset_name}\"\n\n        # Skip if already completed in a previous session\n        if key in completed_combos:\n            print(f\"  [{combo_idx}/{total_combos}] {key} â€” âœ“ already done (loaded from checkpoint)\")\n            continue\n\n        print(f\"\\n{'='*60}\")\n        print(f\"  [{combo_idx}/{total_combos}] {model_type} on {subset_name} ({len(X)} samples)\")\n        print(f\"{'='*60}\")\n        t0 = time.time()\n\n        results = nested_cv(X, y, model_type)\n\n        elapsed = time.time() - t0\n        print(f\"\\n  Results ({elapsed/60:.1f} min):\")\n        print(f\"    RMSE: {results['RMSE_mean']:.3f} Â± {results['RMSE_std']:.3f} MPa\")\n        print(f\"    MAE:  {results['MAE_mean']:.3f} Â± {results['MAE_std']:.3f} MPa\")\n        print(f\"    RÂ²:   {results['R2_mean']:.4f} Â± {results['R2_std']:.4f}\")\n        print(f\"    MAPE: {results['MAPE_mean']:.1f} Â± {results['MAPE_std']:.1f}%\")\n\n        result_row = {\n            'Subset': subset_name, 'Model': model_type,\n            'RMSE_mean': results['RMSE_mean'], 'RMSE_std': results['RMSE_std'],\n            'MAE_mean': results['MAE_mean'],   'MAE_std': results['MAE_std'],\n            'R2_mean': results['R2_mean'],     'R2_std': results['R2_std'],\n        }\n        all_results.append(result_row)\n\n        best_p = results['best_params_per_fold'][0]\n        all_best_params[key] = best_p\n        preds = {'y_true': results['y_true'], 'y_pred': results['y_pred']}\n        all_predictions[key] = preds\n\n        # â”€â”€ CHECKPOINT: Save to Drive immediately â”€â”€\n        checkpoint = {\n            'result_row': result_row,\n            'best_params': best_p,\n            'predictions': preds,\n        }\n        with open(f'{checkpoint_dir}/{key}.json', 'w') as f:\n            json.dump(checkpoint, f, indent=2)\n        print(f\"  ğŸ’¾ Checkpoint saved to Drive: {key}\")\n\ntotal_elapsed = time.time() - pipeline_start\nskipped = len(completed_combos)\ntrained = combo_idx - skipped\nprint(f\"\\n{'='*60}\")\nprint(f\"  âœ… Training complete! Trained: {trained}, Resumed: {skipped}, Total: {total_elapsed/60:.1f} min\")\nprint(f\"{'='*60}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results_df = pd.DataFrame(all_results).round(4)\n\n# Separate table for boosting models\nboost_df = results_df[results_df['Model'].isin(MODEL_TYPES)].copy()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"  GRADIENT BOOSTING MODELS â€” NESTED CV RESULTS\")\nprint(\"=\"*80)\nprint(boost_df.to_string(index=False))\n\n# Best model per subset\nprint(f\"\\n{'â”€'*50}\")\nprint(\"  Best model per subset (by RÂ²):\")\nprint(f\"{'â”€'*50}\")\nfor subset in boost_df['Subset'].unique():\n    sdf = boost_df[boost_df['Subset'] == subset]\n    best = sdf.loc[sdf['R2_mean'].idxmax()]\n    print(f\"  {subset:<6}: {best['Model']:<12} RÂ²={best['R2_mean']:.4f}  RMSE={best['RMSE_mean']:.3f} MPa\")\n\nprint(f\"\\n\\n{'='*80}\")\nprint(\"  ALL MODELS (including baselines)\")\nprint(\"=\"*80)\nprint(results_df.to_string(index=False))\n\n# Save results (local + Drive)\nresults_df.to_csv('outputs/stage_a_results_summary.csv', index=False)\nresults_df.to_csv(f'{DRIVE_DIR}/outputs/stage_a_results_summary.csv', index=False)\nwith open('outputs/best_hyperparameters.json', 'w') as f:\n    json.dump(all_best_params, f, indent=2)\nwith open(f'{DRIVE_DIR}/outputs/best_hyperparameters.json', 'w') as f:\n    json.dump(all_best_params, f, indent=2)\nprint(\"\\nâœ… Results saved to outputs/ and Google Drive\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MODEL_COLORS = {\n    'XGBoost': '#2196F3', 'CatBoost': '#FF5722', 'LightGBM': '#4CAF50',\n    'LinearRegression': '#9E9E9E', 'RandomForest': '#FF9800',\n}\n\n# Prediction scatter plots for boosting models\nfig_rows = len(subsets)\nfig, axes = plt.subplots(fig_rows, 3, figsize=(18, 5.5 * fig_rows))\nif fig_rows == 1:\n    axes = axes.reshape(1, -1)\n\nfor i, subset_name in enumerate(subsets.keys()):\n    for j, model_type in enumerate(MODEL_TYPES):\n        key = f\"{model_type}_{subset_name}\"\n        if key not in all_predictions:\n            continue\n        yt = np.array(all_predictions[key]['y_true'])\n        yp = np.array(all_predictions[key]['y_pred'])\n\n        ax = axes[i, j]\n        color = MODEL_COLORS[model_type]\n        ax.scatter(yt, yp, alpha=0.5, s=30, c=color, edgecolors='white', linewidths=0.3)\n        lims = [min(yt.min(), yp.min())-2, max(yt.max(), yp.max())+2]\n        ax.plot(lims, lims, 'k--', lw=1, alpha=0.6)\n        ax.set_xlim(lims); ax.set_ylim(lims)\n        ax.set_aspect('equal', adjustable='box')\n\n        rmse = np.sqrt(mean_squared_error(yt, yp))\n        r2 = r2_score(yt, yp)\n        ax.set_title(f\"{model_type} â€” {subset_name}\\nRÂ²={r2:.4f}, RMSE={rmse:.2f}\", fontweight='bold', fontsize=11)\n        ax.set_xlabel('Measured (MPa)')\n        ax.set_ylabel('Predicted (MPa)')\n        ax.grid(alpha=0.2)\n\nplt.tight_layout()\nplt.savefig('outputs/prediction_scatter_all.png', dpi=200, bbox_inches='tight')\nplt.show()\n\n# RÂ² heatmap\nif len(subsets) > 1:\n    pivot = boost_df.pivot_table(index='Model', columns='Subset', values='R2_mean')\n    ordered = [c for c in ['EA1','EA7','EA14','Full'] if c in pivot.columns]\n    pivot = pivot[ordered]\n\n    fig, ax = plt.subplots(figsize=(8, 4))\n    sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn', vmin=0.7, vmax=1.0,\n                ax=ax, linewidths=0.5, annot_kws={'fontsize': 13, 'fontweight': 'bold'})\n    ax.set_title('RÂ² Score â€” Models Ã— Subsets', fontweight='bold', fontsize=14)\n    plt.tight_layout()\n    plt.savefig('outputs/r2_heatmap.png', dpi=200, bbox_inches='tight')\n    plt.show()\n\nprint(\"âœ… Plots saved to outputs/\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SHAP Explainability Analysis\n\nComputes SHAP values for each boosting model on the **Full** dataset.\nGenerates:\n- Feature importance bar chart\n- Beeswarm summary plot\n- Dependence plots for top-3 features\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train final models on Full dataset for SHAP\nif 'Full' in subsets:\n    X_full = subsets['Full'][feature_cols]\n    y_full = subsets['Full']['Compressive_Strength']\n\n    for model_type in MODEL_TYPES:\n        key = f\"{model_type}_Full\"\n        if key not in all_best_params:\n            print(f\"âš ï¸  Skipping SHAP for {model_type} (no Full params)\")\n            continue\n\n        print(f\"\\nâ”€â”€ SHAP: {model_type} â”€â”€\")\n        best_p = all_best_params[key]\n        model = create_model(model_type, best_p)\n        model.fit(X_full, y_full)\n\n        explainer = shap.TreeExplainer(model)\n        shap_vals = explainer.shap_values(X_full)\n\n        # Feature importance bar\n        plt.figure(figsize=(10, 7))\n        shap.summary_plot(shap_vals, X_full, plot_type=\"bar\", show=False)\n        plt.title(f\"SHAP Feature Importance â€” {model_type}\", fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig(f\"outputs/shap_importance_{model_type}.png\", dpi=200, bbox_inches='tight')\n        plt.show()\n\n        # Beeswarm\n        plt.figure(figsize=(10, 7))\n        shap.summary_plot(shap_vals, X_full, show=False)\n        plt.title(f\"SHAP Summary â€” {model_type}\", fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig(f\"outputs/shap_summary_{model_type}.png\", dpi=200, bbox_inches='tight')\n        plt.show()\n\n        # Top-3 dependence plots\n        importance = np.abs(shap_vals).mean(axis=0)\n        top3 = list(X_full.columns[np.argsort(importance)[-3:][::-1]])\n        print(f\"  Top-3 features: {top3}\")\n\n        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n        for ax_i, feat in enumerate(top3):\n            plt.sca(axes[ax_i])\n            shap.dependence_plot(feat, shap_vals, X_full, ax=axes[ax_i], show=False)\n            axes[ax_i].set_title(f\"{feat}\", fontweight='bold')\n        plt.suptitle(f\"SHAP Dependence â€” {model_type}\", fontsize=14, fontweight='bold', y=1.02)\n        plt.tight_layout()\n        plt.savefig(f\"outputs/shap_dependence_{model_type}.png\", dpi=200, bbox_inches='tight')\n        plt.show()\nelse:\n    print(\"âš ï¸  Full subset not included â€” skipping SHAP analysis.\")\n    print(\"    Add 'Full' to SUBSET_NAMES and re-run to generate SHAP plots.\")\n\nprint(\"\\nâœ… SHAP analysis complete.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Trained Models for Deployment\n\nSaves all model artifacts needed for inference:\n- Native model files (`.json`, `.cbm`, `.txt`)\n- Universal pickle files (`.pkl`)\n- Feature configuration (`feature_config.json`)\n- Results summary and hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Saving model artifacts for deployment...\\n\")\n\n# Train final models on each subset with best params and save\nsaved_models = {}\n\nfor subset_name, subset_df in subsets.items():\n    X = subset_df[feature_cols]\n    y = subset_df['Compressive_Strength']\n\n    for model_type in MODEL_TYPES:\n        key = f\"{model_type}_{subset_name}\"\n        if key not in all_best_params:\n            continue\n\n        best_p = all_best_params[key]\n        model = create_model(model_type, best_p)\n        model.fit(X, y)\n\n        prefix = f\"models/{model_type}_{subset_name}\"\n\n        # Native format\n        if model_type == 'XGBoost':\n            model.save_model(f\"{prefix}.json\")\n            fmt = \"json\"\n        elif model_type == 'CatBoost':\n            model.save_model(f\"{prefix}.cbm\")\n            fmt = \"cbm\"\n        elif model_type == 'LightGBM':\n            model.booster_.save_model(f\"{prefix}.txt\")\n            fmt = \"txt\"\n\n        # Universal pickle\n        joblib.dump(model, f\"{prefix}.pkl\")\n\n        saved_models[key] = {'native': f\"{prefix}.{fmt}\", 'pkl': f\"{prefix}.pkl\"}\n\n        # Also copy to Drive\n        shutil.copy(f\"{prefix}.{fmt}\", f\"{DRIVE_DIR}/models/{model_type}_{subset_name}.{fmt}\")\n        shutil.copy(f\"{prefix}.pkl\", f\"{DRIVE_DIR}/models/{model_type}_{subset_name}.pkl\")\n        print(f\"  âœ… {key}: saved (.{fmt} + .pkl) â†’ Drive\")\n\n# Feature configuration\nfeature_config = {\n    'raw_features': ['Cement', 'Blast_Furnace_Slag', 'Fly_Ash', 'Water',\n                     'Superplasticizer', 'Coarse_Aggregate', 'Fine_Aggregate', 'Age'],\n    'all_features': feature_cols,\n    'target': 'Compressive_Strength',\n    'n_engineered': len(feature_cols) - 8,\n    'subsets_trained': list(subsets.keys()),\n    'models_trained': MODEL_TYPES,\n    'saved_models': saved_models,\n    'best_hyperparameters': all_best_params,\n}\n\nwith open('models/feature_config.json', 'w') as f:\n    json.dump(feature_config, f, indent=2)\nshutil.copy('models/feature_config.json', f'{DRIVE_DIR}/models/feature_config.json')\nprint(f\"\\n  âœ… Feature config saved to models/ and Drive\")\n\n# Copy results\nshutil.copy('outputs/stage_a_results_summary.csv', 'models/stage_a_results_summary.csv')\nshutil.copy('outputs/best_hyperparameters.json', 'models/best_hyperparameters.json')\n\n# Sync everything to Drive\nfor fname in os.listdir('models'):\n    src = f'models/{fname}'\n    dst = f'{DRIVE_DIR}/models/{fname}'\n    if not os.path.exists(dst):\n        shutil.copy(src, dst)\n\nprint(f\"\\nğŸ“¦ All artifacts saved to models/\")\nprint(f\"   Total files: {len(os.listdir('models'))}\")\nfor f in sorted(os.listdir('models')):\n    size = os.path.getsize(f'models/{f}')\n    print(f\"   {f:<45s} {size/1024:.1f} KB\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Download Model Artifacts\n\nDownloads a ZIP file containing all trained models, configs, and results.\nUse these files in your local Streamlit app for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create ZIP of all artifacts\nshutil.make_archive('Stage_A_Models', 'zip', '.', 'models')\n\n# Also create a ZIP of outputs (plots)\nshutil.make_archive('Stage_A_Outputs', 'zip', '.', 'outputs')\n\nprint(\"ğŸ“¥ Downloading model artifacts...\")\nfrom google.colab import files\nfiles.download('Stage_A_Models.zip')\n\nprint(\"\\nğŸ“¥ Downloading output plots...\")\nfiles.download('Stage_A_Outputs.zip')\n\nprint(\"\\nâœ… Done! Unzip Stage_A_Models.zip locally for inference.\")\nprint(\"   The models/ directory contains everything needed for the Streamlit app.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}